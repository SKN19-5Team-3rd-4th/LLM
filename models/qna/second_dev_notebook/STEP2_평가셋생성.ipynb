{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0452a8d5",
   "metadata": {},
   "source": [
    "- 사용 모델 : gpt-4o-mini\n",
    "- 각 평가 질의에 대해 다양한 표현의 질문 변형"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe2ce4a",
   "metadata": {},
   "source": [
    "#### 평가셋 랜덤 추출\n",
    "- eval_queries.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f68a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 작업 디렉토리: c:\\SKN_19\\project\\llm\\cache\n",
      "\n",
      "카테고리 분포:\n",
      "  symptom_diagnosis: 416\n",
      "  environment_recommendation: 96\n",
      "  plant_identification: 94\n",
      "  repotting: 55\n",
      "  urgent_care: 52\n",
      "  propagation: 45\n",
      "  blooming_fruiting: 35\n",
      "  pruning_shaping: 23\n",
      "  beginner_faq: 4\n",
      "  other: 3\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "notebook_dir = Path.cwd()\n",
    "if notebook_dir.name != 'cache':\n",
    "    os.chdir(Path(__file__).parent if '__file__' in globals() else Path.cwd())\n",
    "print(f\"현재 작업 디렉토리: {os.getcwd()}\")\n",
    "\n",
    "\n",
    "with open('post_preprocessed_data_v3.json', 'r', encoding='utf-8') as f:\n",
    "    documents = json.load(f)\n",
    "category_counts = Counter([d['category_main'] for d in documents if not d['exclude']])\n",
    "print(\"\\n카테고리 분포:\")\n",
    "for cat, count in category_counts.most_common():\n",
    "    print(f\"  {cat}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69f6548f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VectorStore 초기화 완료 (namespace: plant-qna-v3-openai)\n"
     ]
    }
   ],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from pinecone import Pinecone\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "QNA_INDEX_NAME = \"plant-qna-v3\"\n",
    "DIMENSION = 1536  \n",
    "UPSERT_NAMESPACE = f\"{QNA_INDEX_NAME}-openai\"\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "index = pc.Index(QNA_INDEX_NAME)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "vector_store = PineconeVectorStore(\n",
    "    index=index,\n",
    "    embedding=embeddings,\n",
    "    namespace=UPSERT_NAMESPACE\n",
    ")\n",
    "print(f\"VectorStore 초기화 완료 (namespace: {UPSERT_NAMESPACE})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e05a7218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "활성 문서 수: 823\n",
      "총 평가 샘플 수: 84\n",
      "\n",
      "카테고리별 샘플 분포:\n",
      "  symptom_diagnosis: 10\n",
      "  environment_recommendation: 10\n",
      "  plant_identification: 10\n",
      "  blooming_fruiting: 10\n",
      "  repotting: 10\n",
      "  propagation: 10\n",
      "  urgent_care: 10\n",
      "  pruning_shaping: 10\n",
      "  beginner_faq: 4\n"
     ]
    }
   ],
   "source": [
    "# 평가용 질의셋 생성\n",
    "# 각 카테고리에서 대표적인 쿼리를 추출하여 평가 샘플 생성\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "   \n",
    "active_docs = [d for d in documents if not d['exclude']]\n",
    "print(f\"활성 문서 수: {len(active_docs)}\")\n",
    "\n",
    "category_docs = defaultdict(list)\n",
    "for i, doc in enumerate(active_docs):\n",
    "    category_docs[doc['category_main']].append((i, doc))\n",
    "\n",
    "# 각 카테고리에서 평가 샘플 추출\n",
    "eval_samples = []\n",
    "samples_per_category = {\n",
    "    'symptom_diagnosis': 10,\n",
    "    'environment_recommendation': 10,\n",
    "    'plant_identification': 10,\n",
    "    'blooming_fruiting': 10,\n",
    "    'repotting': 10,\n",
    "    'propagation': 10,\n",
    "    'urgent_care': 10,\n",
    "    'pruning_shaping': 10,\n",
    "    'beginner_faq': 4\n",
    "}\n",
    "   \n",
    "for category, count in samples_per_category.items():\n",
    "    if category in category_docs:\n",
    "        available = category_docs[category]\n",
    "        sample_size = min(count, len(available))\n",
    "        sampled = random.sample(available, sample_size)\n",
    "        \n",
    "        for idx, doc in sampled:\n",
    "            eval_samples.append({\n",
    "                'query': doc['question'],\n",
    "                'gold_ids': [doc['ids']],\n",
    "                'category_main': category,\n",
    "                'doc_index': idx,\n",
    "                'post_id': doc['base_metadata']['post_id'],\n",
    "                'channel': doc['base_metadata']['channel'],\n",
    "                'date': doc['base_metadata']['date']\n",
    "            })\n",
    "\n",
    "print(f\"총 평가 샘플 수: {len(eval_samples)}\")\n",
    "print(\"\\n카테고리별 샘플 분포:\")\n",
    "sample_category_counts = Counter([s['category_main'] for s in eval_samples])\n",
    "for cat, count in sample_category_counts.most_common():\n",
    "    print(f\"  {cat}: {count}\")\n",
    "\n",
    "# 평가셋 저장\n",
    "eval_set_path = 'eval_queries.json'\n",
    "with open(eval_set_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(eval_samples, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b758e7",
   "metadata": {},
   "source": [
    "#### 질문 변형 평가셋 생성\n",
    "- augmented_eval_queries_for_rag.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f5c1a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 문서 수: 823\n",
      "평가 샘플 수: 84\n",
      "문서 ID 매핑 완료: 807개\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "with open('post_preprocessed_data_v3.json', 'r', encoding='utf-8') as f:\n",
    "    all_docs = json.load(f)\n",
    "\n",
    "with open('eval_queries.json', 'r', encoding='utf-8') as f:\n",
    "    eval_samples = json.load(f)\n",
    "\n",
    "docs_by_id = {doc['ids']: doc for doc in all_docs}\n",
    "category_docs = defaultdict(list)\n",
    "for doc in all_docs:\n",
    "    if not doc.get('exclude', False):\n",
    "        category_docs[doc['category_main']].append(doc)\n",
    "\n",
    "print(f\"전체 문서 수: {len(all_docs)}\")\n",
    "print(f\"평가 샘플 수: {len(eval_samples)}\")\n",
    "print(f\"문서 ID 매핑 완료: {len(docs_by_id)}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36c6adba",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "##당신의 역할:\n",
    "- 당신은 \"식물 상담 Q&A 서비스\"의 RAG 평가셋을 고도화하는 데이터 큐레이터입니다.\n",
    "- 각 기준 문서(base_doc)와 그와 연관될 수 있는 후보 문서들(candidate_docs)을 입력으로 받아,\n",
    "  1) 질문 변형(query paraphrase)을 여러 개 만들고,\n",
    "  2) gold_ids를 하나에서 여러 개로 확장하는 일을 합니다.\n",
    "\n",
    "##목표:\n",
    "- 현재 평가셋은 `query = 원문 question`, `gold_ids = [해당 문서 1개]`로 구성되어 있습니다.\n",
    "- 당신은 이를 다음과 같이 더 현실적인 평가셋으로 변환해야 합니다.\n",
    "  1) 기준 질문과 의미는 같지만 표현이 다른 query를 여러 개 생성\n",
    "  2) 같은 증상/상황/정보를 다루는 다른 Q&A 문서들도 gold_ids에 포함\n",
    "\n",
    "##주의사항:\n",
    "- 답변은 **반드시 JSON 형식**으로만 출력합니다.\n",
    "- JSON 이외의 설명, 코멘트는 절대 포함하지 마세요.\n",
    "- Boolean이 아니라 문자열/리스트 위주이므로, null 대신 빈 문자열이나 빈 배열을 사용해도 됩니다.\n",
    "\n",
    "[해야 할 일 1: 질문 변형 만들기]\n",
    "\n",
    "1. base_doc.question을 기반으로, 다음 조건을 만족하는 query 변형들을 만듭니다.\n",
    "   - 의미/의도는 유지하되, 표현/문장 구조/단어 선택을 바꿀 것\n",
    "   - 실제 사용자가 질문할 법한 자연스러운 한국어 문장일 것\n",
    "   - 가능한 한 다음과 같은 변화를 섞어서 만들어도 좋습니다.\n",
    "     - 구어체 ↔ 조금 더 정중한 말투\n",
    "     - 순서 바꾸기 (증상 → 환경 / 환경 → 증상)\n",
    "     - \"원인에 대한 가설(과습/영양부족/햇빛 등)\"을 질문에 포함시키거나 빼보기\n",
    "     - 단, 증상/식물/핵심 조건 자체는 바꾸지 말 것\n",
    "\n",
    "2. 생성할 query 개수:\n",
    "   - 최소 2개, 최대 4개 정도\n",
    "   - 그 중 1개는 base_doc.question을 그대로 사용해도 됩니다.\n",
    "\n",
    "[해야 할 일 2: gold_ids 여러 개로 확장]\n",
    "\n",
    "1. base_doc는 항상 gold_ids에 포함합니다.\n",
    "   - 즉, 기본적으로 gold_ids에는 `\"base_doc.ids\"`가 들어가야 합니다.\n",
    "\n",
    "2. candidate_docs 목록을 모두 검토하고,\n",
    "   - base_doc의 질문/답변/상황과 **실질적으로 같은 문제를 다루거나,**\n",
    "   - 증상과 환경이 매우 유사해서 \"이 문서들을 함께 참고해도 좋다\"고 판단되는 경우,\n",
    "   - 해당 candidate의 ids를 gold_ids에 추가합니다.\n",
    "\n",
    "3. gold_ids에 포함될 수 있는 기준 예시:\n",
    "   - 동일한 식물/유사한 식물 (예: 둘 다 몬스테라, 혹은 둘 다 관엽/비슷한 환경 요구)\n",
    "   - 증상 패턴이 거의 동일 (예: 잎 끝 갈변, 과습 의심, 겨울철 실내 환경 등)\n",
    "   - 추천 조치가 유사 (예: 물주기 줄이기, 간접광으로 이동, 통풍 개선 등)\n",
    "\n",
    "4. gold_ids에 포함되지 않아야 할 예시:\n",
    "   - 전혀 다른 증상/문제 (예: 하나는 해충, 하나는 냉해)\n",
    "   - 다른 도메인(예: 환경 추천 vs 식별 질문)만 다루는 문서\n",
    "   - 식물/환경/증상이 너무 달라서, 이 query에 대한 정답으로 보기 어렵다고 판단되는 문서\n",
    "\n",
    "5. gold_ids는 중복 없이 배열로 정리합니다.\n",
    "   - base_doc.ids를 맨 앞에 두고, 나머지 candidate ids를 뒤에 나열합니다.\n",
    "\n",
    "[출력 형식]\n",
    "\n",
    "당신은 아래 형식의 JSON 객체 하나를 출력합니다.\n",
    "\n",
    "{\n",
    "  \"base_id\": string,                 // base_doc.ids\n",
    "  \"category_main\": string,           // base_doc.category_main\n",
    "  \"base_question\": string,           // base_doc.question\n",
    "  \"eval_queries\": [\n",
    "    {\n",
    "      \"query\": string,               // 변형된 질문 1개\n",
    "      \"gold_ids\": [string, ...]      // base_id + 관련 candidate ids\n",
    "    },\n",
    "    {\n",
    "      \"query\": string,\n",
    "      \"gold_ids\": [string, ...]\n",
    "    }\n",
    "    ...\n",
    "  ]\n",
    "}\n",
    "\n",
    "###규칙:\n",
    "- eval_queries 배열에는 최소 2개, 최대 4개의 항목을 만듭니다.\n",
    "- 각 eval_queries[*].gold_ids는 모두 같은 리스트여도 괜찮지만,\n",
    "  - 필요하다면 query의 뉘앙스에 맞춰 조금 다르게 구성해도 됩니다.\n",
    "- base_id는 항상 gold_ids에 포함되어야 합니다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df034541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_candidate_docs(base_doc, all_docs_list, num_candidates=15):\n",
    "    \"\"\"\n",
    "    base_doc와 관련된 candidate 문서들을 선정\n",
    "    \n",
    "    선정 기준:\n",
    "    1. 같은 카테고리 (category_main)\n",
    "    2. 겹치는 서브카테고리 (category_sub)\n",
    "    3. base_doc 자신은 제외\n",
    "    \"\"\"\n",
    "    base_id = base_doc['ids']\n",
    "    base_category = base_doc['category_main']\n",
    "    base_sub_categories = set(base_doc.get('category_sub', []))\n",
    "    \n",
    "    candidates = []\n",
    "    \n",
    "    for doc in all_docs_list:\n",
    "        # base_doc 자신은 제외\n",
    "        if doc['ids'] == base_id:\n",
    "            continue\n",
    "        \n",
    "        # exclude된 문서 제외\n",
    "        if doc.get('exclude', False):\n",
    "            continue\n",
    "        \n",
    "        # 점수 계산\n",
    "        score = 0\n",
    "        \n",
    "        # 같은 메인 카테고리면 점수 +10\n",
    "        if doc['category_main'] == base_category:\n",
    "            score += 10\n",
    "        \n",
    "        # 서브 카테고리 겹치는 개수만큼 점수 추가\n",
    "        doc_sub_categories = set(doc.get('category_sub', []))\n",
    "        overlap = base_sub_categories & doc_sub_categories\n",
    "        score += len(overlap) * 5\n",
    "        \n",
    "        if score > 0:\n",
    "            candidates.append((score, doc))\n",
    "    \n",
    "    # 점수 높은 순으로 정렬하고 상위 num_candidates개 선택\n",
    "    candidates.sort(key=lambda x: x[0], reverse=True)\n",
    "    selected_candidates = [doc for score, doc in candidates[:num_candidates]]\n",
    "    \n",
    "    return selected_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcbaa906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문 변형 평가용 정답 셋 생성 \n",
    "def call_llm_for_augmentation(base_doc, candidate_docs):\n",
    "    \n",
    "    # 입력 데이터 구성\n",
    "    input_data = {\n",
    "        \"base_doc\": {\n",
    "            \"ids\": base_doc['ids'],\n",
    "            \"question\": base_doc['question'],\n",
    "            \"answer\": base_doc['answer'],\n",
    "            \"category_main\": base_doc['category_main'],\n",
    "            \"category_sub\": base_doc.get('category_sub', []),\n",
    "            \"base_metadata\": base_doc.get('base_metadata', {})\n",
    "        },\n",
    "        \"candidate_docs\": [\n",
    "            {\n",
    "                \"ids\": doc['ids'],\n",
    "                \"question\": doc['question'],\n",
    "                \"answer\": doc['answer'],\n",
    "                \"category_main\": doc['category_main'],\n",
    "                \"category_sub\": doc.get('category_sub', [])\n",
    "            }\n",
    "            for doc in candidate_docs\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",  \n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": json.dumps(input_data, ensure_ascii=False, indent=2)}\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        \n",
    "        result = json.loads(response.choices[0].message.content)\n",
    "        return result\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {base_doc['ids']}: {str(e)}\")\n",
    "        \n",
    "        # 에러 발생 시 기본 구조 반환\n",
    "        return {\n",
    "            \"base_id\": base_doc['ids'],\n",
    "            \"category_main\": base_doc['category_main'],\n",
    "            \"base_question\": base_doc['question'],\n",
    "            \"eval_queries\": [\n",
    "                {\n",
    "                    \"query\": base_doc['question'],\n",
    "                    \"gold_ids\": [base_doc['ids']]\n",
    "                }\n",
    "            ],\n",
    "            \"error\": str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40cf1a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_eval_samples(eval_samples, all_docs, num_samples=None):\n",
    "    \"\"\"\n",
    "    평가 샘플들을 처리하여 고도화된 평가셋 생성\n",
    "    \n",
    "    Args:\n",
    "        eval_samples: 기존 평가 샘플 리스트\n",
    "        all_docs: 전체 문서 리스트\n",
    "        num_samples: 처리할 샘플 수 (None이면 전체)\n",
    "    \"\"\"\n",
    "    augmented_results = []\n",
    "    \n",
    "    # 처리할 샘플 수 결정\n",
    "    samples_to_process = eval_samples[:num_samples] if num_samples else eval_samples\n",
    "    \n",
    "    print(f\"총 {len(samples_to_process)}개의 평가 샘플 처리 시작...\")\n",
    "    \n",
    "    for idx, eval_sample in enumerate(tqdm(samples_to_process)):\n",
    "        # gold_ids의 첫 번째 ID가 base_doc\n",
    "        base_id = eval_sample['gold_ids'][0]\n",
    "        \n",
    "        # base_doc 찾기\n",
    "        if base_id not in docs_by_id:\n",
    "            print(f\"Warning: {base_id} not found in docs_by_id\")\n",
    "            continue\n",
    "        \n",
    "        base_doc = docs_by_id[base_id]\n",
    "        \n",
    "        # candidate 문서 선정\n",
    "        candidate_docs = select_candidate_docs(base_doc, all_docs, num_candidates=15)\n",
    "        \n",
    "        # LLM API 호출\n",
    "        result = call_llm_for_augmentation(base_doc, candidate_docs)\n",
    "        \n",
    "        # 메타데이터 추가\n",
    "        result['original_eval_sample'] = eval_sample\n",
    "        result['num_candidates_provided'] = len(candidate_docs)\n",
    "        \n",
    "        augmented_results.append(result)\n",
    "        \n",
    "        # 중간 저장 (10개마다)\n",
    "        if (idx + 1) % 10 == 0:\n",
    "            with open('augmented_eval_temp.json', 'w', encoding='utf-8') as f:\n",
    "                json.dump(augmented_results, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\n처리 완료: {len(augmented_results)}개\")\n",
    "    return augmented_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92d2db53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_augmented_eval(augmented_results):\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"평가셋 분석\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    total_samples = len(augmented_results)\n",
    "    total_queries = sum(len(r['eval_queries']) for r in augmented_results)\n",
    "    \n",
    "    print(f\"\\n기본 통계:\")\n",
    "    print(f\"  - 총 base 문서 수: {total_samples}\")\n",
    "    print(f\"  - 총 생성된 query 변형 수: {total_queries}\")\n",
    "    print(f\"  - base 문서당 평균 query 수: {total_queries / total_samples:.2f}\")\n",
    "    \n",
    "    # gold_ids 개수 분석\n",
    "    gold_ids_counts = []\n",
    "    for result in augmented_results:\n",
    "        for eq in result['eval_queries']:\n",
    "            gold_ids_counts.append(len(eq['gold_ids']))\n",
    "    \n",
    "    print(f\"\\ngold_ids 확장 통계:\")\n",
    "    print(f\"  - 평균 gold_ids 개수: {sum(gold_ids_counts) / len(gold_ids_counts):.2f}\")\n",
    "    print(f\"  - 최소 gold_ids 개수: {min(gold_ids_counts)}\")\n",
    "    print(f\"  - 최대 gold_ids 개수: {max(gold_ids_counts)}\")\n",
    "    \n",
    "    # gold_ids 개수별 분포\n",
    "    from collections import Counter\n",
    "    gold_ids_distribution = Counter(gold_ids_counts)\n",
    "    print(f\"\\ngold_ids 개수 분포:\")\n",
    "    for count in sorted(gold_ids_distribution.keys()):\n",
    "        print(f\"  - {count}개: {gold_ids_distribution[count]}건\")\n",
    "    \n",
    "    # 카테고리별 분포\n",
    "    category_counts = Counter([r['category_main'] for r in augmented_results])\n",
    "    print(f\"\\n카테고리별 분포:\")\n",
    "    for cat, count in category_counts.most_common():\n",
    "        print(f\"  - {cat}: {count}건\")\n",
    "    \n",
    "    # 에러 체크\n",
    "    errors = [r for r in augmented_results if 'error' in r]\n",
    "    if errors:\n",
    "        print(f\"\\n에러 발생: {len(errors)}건\")\n",
    "        for err in errors[:3]:  # 처음 3개만 표시\n",
    "            print(f\"  - {err['base_id']}: {err.get('error', 'Unknown')}\")\n",
    "    else:\n",
    "        print(f\"\\n모든 샘플 정상 처리\")\n",
    "    \n",
    "    return {\n",
    "        'total_samples': total_samples,\n",
    "        'total_queries': total_queries,\n",
    "        'avg_queries_per_sample': total_queries / total_samples,\n",
    "        'avg_gold_ids': sum(gold_ids_counts) / len(gold_ids_counts),\n",
    "        'gold_ids_distribution': dict(gold_ids_distribution),\n",
    "        'category_distribution': dict(category_counts),\n",
    "        'num_errors': len(errors)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0d23d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과를 RAG 평가에 사용할 형식으로 변환\n",
    "def convert_to_eval_format(augmented_results):\n",
    "    \"\"\"\n",
    "    고도화된 결과를 RAG 평가 시스템에서 사용할 형식으로 변환\n",
    "    \n",
    "    출력 형식:\n",
    "    [\n",
    "      {\n",
    "        \"query\": \"질문 변형 1\",\n",
    "        \"gold_ids\": [\"groro_3300\", \"groro_3329\"],\n",
    "        \"category_main\": \"symptom_diagnosis\",\n",
    "        \"base_doc_id\": \"groro_3300\",\n",
    "        \"query_variant_idx\": 0\n",
    "      },\n",
    "      ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    eval_format = []\n",
    "    \n",
    "    for result in augmented_results:\n",
    "        base_id = result['base_id']\n",
    "        category_main = result['category_main']\n",
    "        \n",
    "        for idx, eq in enumerate(result['eval_queries']):\n",
    "            eval_format.append({\n",
    "                'query': eq['query'],\n",
    "                'gold_ids': eq['gold_ids'],\n",
    "                'category_main': category_main,\n",
    "                'base_doc_id': base_id,\n",
    "                'query_variant_idx': idx\n",
    "            })\n",
    "    \n",
    "    return eval_format\n",
    "\n",
    "\n",
    "# 생성된 결과의 샘플(유효성) 검증\n",
    "def validate_augmented_results(augmented_results, docs_by_id):\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"결과 검증\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    issues = []\n",
    "    \n",
    "    for idx, result in enumerate(augmented_results):\n",
    "        base_id = result['base_id']\n",
    "        \n",
    "        # 1. base_id가 존재하는지 확인\n",
    "        if base_id not in docs_by_id:\n",
    "            issues.append(f\"Result {idx}: base_id '{base_id}' not found in docs\")\n",
    "        \n",
    "        # 2. eval_queries가 비어있지 않은지 확인\n",
    "        if not result.get('eval_queries'):\n",
    "            issues.append(f\"Result {idx}: eval_queries is empty\")\n",
    "            continue\n",
    "        \n",
    "        # 3. 각 query에 대해 검증\n",
    "        for q_idx, eq in enumerate(result['eval_queries']):\n",
    "            # query가 비어있지 않은지\n",
    "            if not eq.get('query') or not eq['query'].strip():\n",
    "                issues.append(f\"Result {idx}, Query {q_idx}: query is empty\")\n",
    "            \n",
    "            # gold_ids가 비어있지 않은지\n",
    "            if not eq.get('gold_ids') or len(eq['gold_ids']) == 0:\n",
    "                issues.append(f\"Result {idx}, Query {q_idx}: gold_ids is empty\")\n",
    "                continue\n",
    "            \n",
    "            # base_id가 gold_ids에 포함되어 있는지\n",
    "            if base_id not in eq['gold_ids']:\n",
    "                issues.append(f\"Result {idx}, Query {q_idx}: base_id not in gold_ids\")\n",
    "            \n",
    "            # 모든 gold_ids가 유효한지 확인\n",
    "            for gold_id in eq['gold_ids']:\n",
    "                if gold_id not in docs_by_id:\n",
    "                    issues.append(f\"Result {idx}, Query {q_idx}: gold_id '{gold_id}' not found\")\n",
    "    \n",
    "    if issues:\n",
    "        print(f\"\\n발견된 문제: {len(issues)}건\\n\")\n",
    "        for issue in issues[:10]:  # 처음 10개만 표시\n",
    "            print(f\"  - {issue}\")\n",
    "        if len(issues) > 10:\n",
    "            print(f\"  ... 외 {len(issues) - 10}건\")\n",
    "    else:\n",
    "        print(f\"\\모든 검증 통과\")\n",
    "    \n",
    "    return issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b1a35e",
   "metadata": {},
   "source": [
    "- 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "926624f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 84개의 평가 샘플 처리 시작...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [13:47<00:00,  9.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "처리 완료: 84개\n",
      "============================================================\n",
      "결과 검증\n",
      "============================================================\n",
      "\n",
      "발견된 문제: 8건\n",
      "\n",
      "  - Result 5, Query 1: gold_id 'groro_5764' not found\n",
      "  - Result 8, Query 3: gold_id 'groro_4401' not found\n",
      "  - Result 16, Query 0: gold_id 'groro_7360' not found\n",
      "  - Result 39, Query 0: gold_id 'groro_6540' not found\n",
      "  - Result 39, Query 1: gold_id 'groro_6540' not found\n",
      "  - Result 39, Query 2: gold_id 'groro_6540' not found\n",
      "  - Result 39, Query 3: gold_id 'groro_6540' not found\n",
      "  - Result 74, Query 3: gold_id 'groro_6836' not found\n",
      "============================================================\n",
      "평가셋 분석\n",
      "============================================================\n",
      "\n",
      "기본 통계:\n",
      "  - 총 base 문서 수: 84\n",
      "  - 총 생성된 query 변형 수: 294\n",
      "  - base 문서당 평균 query 수: 3.50\n",
      "\n",
      "gold_ids 확장 통계:\n",
      "  - 평균 gold_ids 개수: 3.24\n",
      "  - 최소 gold_ids 개수: 1\n",
      "  - 최대 gold_ids 개수: 4\n",
      "\n",
      "gold_ids 개수 분포:\n",
      "  - 1개: 8건\n",
      "  - 2개: 16건\n",
      "  - 3개: 168건\n",
      "  - 4개: 102건\n",
      "\n",
      "카테고리별 분포:\n",
      "  - symptom_diagnosis: 10건\n",
      "  - environment_recommendation: 10건\n",
      "  - plant_identification: 10건\n",
      "  - blooming_fruiting: 10건\n",
      "  - repotting: 10건\n",
      "  - propagation: 10건\n",
      "  - urgent_care: 10건\n",
      "  - pruning_shaping: 10건\n",
      "  - beginner_faq: 4건\n",
      "\n",
      "모든 샘플 정상 처리\n",
      "\\n전체 결과가 저장되었습니다:\n",
      "  - 원본 형식: augmented_eval_queries_full.json\n",
      "  - RAG 평가용: augmented_eval_queries_for_rag.json\n",
      "  - 총 평가 쿼리 수: 294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 전체 실행\n",
    "augmented_eval_full = process_eval_samples(eval_samples, all_docs, num_samples=None)\n",
    "\n",
    "# 검증\n",
    "issues = validate_augmented_results(augmented_eval_full, docs_by_id)\n",
    "\n",
    "# 분석\n",
    "stats = analyze_augmented_eval(augmented_eval_full)\n",
    "\n",
    "# 저장 (원본 형식)\n",
    "with open('augmented_eval_queries_full.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(augmented_eval_full, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# 평가용 형식으로 변환 및 저장\n",
    "eval_format = convert_to_eval_format(augmented_eval_full)\n",
    "with open('augmented_eval_queries_for_rag.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(eval_format, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\\\n전체 결과가 저장되었습니다:\")\n",
    "print(f\"  - 원본 형식: augmented_eval_queries_full.json\")\n",
    "print(f\"  - RAG 평가용: augmented_eval_queries_for_rag.json\")\n",
    "print(f\"  - 총 평가 쿼리 수: {len(eval_format)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32829f01",
   "metadata": {},
   "source": [
    "#### jhgan/ko-sroberta-multitask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd7535e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "# # 두 개의 QnA 인덱스 설정\n",
    "# INDEX_V1_NAME = \"plant-qna\"     # 기존 인덱스\n",
    "# INDEX_V3_NAME = \"plant-qna-v3\"  # 메타데이터 추가 인덱스\n",
    "\n",
    "# # 각 인덱스에 대한 벡터 스토어 및 retriever 생성\n",
    "# index_v1 = pc.Index(INDEX_V1_NAME)\n",
    "# vector_store_v1 = PineconeVectorStore(index=index_v1, embedding=embeddings)\n",
    "# retriever_v1 = vector_store_v1.as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "# index_v3 = pc.Index(INDEX_V3_NAME)\n",
    "# vector_store_v3 = PineconeVectorStore(index=index_v3, embedding=embeddings)\n",
    "# retriever_v3 = vector_store_v3.as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "# print(f\"{INDEX_V1_NAME} retriever 준비 완료\")\n",
    "# print(f\"{INDEX_V3_NAME} retriever 준비 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10326a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 평가셋 로드\n",
    "# with open('augmented_eval_queries_for_rag.json', 'r', encoding='utf-8') as f:\n",
    "#     eval_queries = json.load(f)\n",
    "\n",
    "# print(f\"{len(eval_queries)}개 쿼리\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2757fc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 평가 함수 정의\n",
    "# def evaluate_retriever(retriever, eval_queries, k_values=[1, 3, 5, 10]):\n",
    "#     \"\"\"\n",
    "#     Retriever의 성능을 평가합니다.\n",
    "    \n",
    "#     Args:\n",
    "#         retriever: LangChain retriever 객체\n",
    "#         eval_queries: 평가 쿼리 리스트\n",
    "#         k_values: 평가할 k 값들\n",
    "        \n",
    "#     Returns:\n",
    "#         results: 각 쿼리별 검색 결과\n",
    "#         metrics: 평가 지표\n",
    "#     \"\"\"\n",
    "#     results = []\n",
    "    \n",
    "#     for query_data in tqdm(eval_queries, desc=\"검색 수행 중\"):\n",
    "#         query = query_data['query']\n",
    "#         gold_ids = query_data['gold_ids']\n",
    "        \n",
    "#         # 검색 수행\n",
    "#         retrieved_docs = retriever.invoke(query)\n",
    "        \n",
    "#         # 검색된 문서 ID 추출\n",
    "#         retrieved_ids = []\n",
    "#         for doc in retrieved_docs:\n",
    "#             doc_id = doc.metadata.get('id', '')\n",
    "#             retrieved_ids.append(doc_id)\n",
    "        \n",
    "#         results.append({\n",
    "#             'query': query,\n",
    "#             'gold_ids': gold_ids,\n",
    "#             'retrieved_ids': retrieved_ids,\n",
    "#             'retrieved_docs': retrieved_docs\n",
    "#         })\n",
    "    \n",
    "#     # 평가 지표 계산\n",
    "#     metrics = calculate_metrics(results, k_values)\n",
    "    \n",
    "#     return results, metrics\n",
    "\n",
    "\n",
    "# def calculate_metrics(results, k_values=[1, 3, 5, 10]):\n",
    "#     \"\"\"\n",
    "#     검색 결과에 대한 평가 지표를 계산합니다.\n",
    "#     \"\"\"\n",
    "#     metrics = {k: {'precision': [], 'recall': [], 'hit_rate': []} for k in k_values}\n",
    "#     mrr_scores = []\n",
    "    \n",
    "#     for result in results:\n",
    "#         gold_ids = set(result['gold_ids'])\n",
    "#         retrieved_ids = result['retrieved_ids']\n",
    "        \n",
    "#         # MRR 계산\n",
    "#         for idx, doc_id in enumerate(retrieved_ids, 1):\n",
    "#             if doc_id in gold_ids:\n",
    "#                 mrr_scores.append(1.0 / idx)\n",
    "#                 break\n",
    "#         else:\n",
    "#             mrr_scores.append(0.0)\n",
    "        \n",
    "#         # k별 지표 계산\n",
    "#         for k in k_values:\n",
    "#             retrieved_k = set(retrieved_ids[:k])\n",
    "            \n",
    "#             # Precision@k\n",
    "#             if len(retrieved_k) > 0:\n",
    "#                 precision = len(retrieved_k & gold_ids) / len(retrieved_k)\n",
    "#             else:\n",
    "#                 precision = 0.0\n",
    "            \n",
    "#             # Recall@k\n",
    "#             if len(gold_ids) > 0:\n",
    "#                 recall = len(retrieved_k & gold_ids) / len(gold_ids)\n",
    "#             else:\n",
    "#                 recall = 0.0\n",
    "            \n",
    "#             # Hit Rate@k\n",
    "#             hit = 1.0 if len(retrieved_k & gold_ids) > 0 else 0.0\n",
    "            \n",
    "#             metrics[k]['precision'].append(precision)\n",
    "#             metrics[k]['recall'].append(recall)\n",
    "#             metrics[k]['hit_rate'].append(hit)\n",
    "    \n",
    "#     # 평균 계산\n",
    "#     summary = {'MRR': np.mean(mrr_scores)}\n",
    "#     for k in k_values:\n",
    "#         summary[f'Precision@{k}'] = np.mean(metrics[k]['precision'])\n",
    "#         summary[f'Recall@{k}'] = np.mean(metrics[k]['recall'])\n",
    "#         summary[f'HitRate@{k}'] = np.mean(metrics[k]['hit_rate'])\n",
    "    \n",
    "#     return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768975fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plant-qna (기존 인덱스) 평가\n",
    "# print(f\"{'='*50}\")\n",
    "# print(f\"{INDEX_V1_NAME} 평가 시작\")\n",
    "# print(f\"{'='*50}\")\n",
    "\n",
    "# # results_v1, metrics_v1 = evaluate_retriever(retriever_v1, eval_queries)\n",
    "# results_v1, metrics_v1 = evaluate_retriever_improved(retriever_v1, eval_queries)\n",
    "\n",
    "# print(\"\\n평가 완료\")\n",
    "# print(\"\\n=== 평가 결과 ===\")\n",
    "# for metric, value in metrics_v1.items():\n",
    "#     print(f\"{metric}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb9e49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plant-qna-v3 (메타데이터 추가 인덱스) 평가\n",
    "# print(f\"{'='*50}\")\n",
    "# print(f\"{INDEX_V3_NAME} 평가 시작\")\n",
    "# print(f\"{'='*50}\")\n",
    "\n",
    "# # results_v3, metrics_v3 = evaluate_retriever(retriever_v3, eval_queries)\n",
    "# results_v3, metrics_v3 = evaluate_retriever_improved(retriever_v3, eval_queries)\n",
    "\n",
    "# print(\"\\n평가 완료\")\n",
    "# print(\"\\n=== 평가 결과 ===\")\n",
    "# for metric, value in metrics_v3.items():\n",
    "#     print(f\"{metric}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80e2eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 결과 비교 데이터프레임 생성\n",
    "# comparison_df = pd.DataFrame({\n",
    "#     'Metric': list(metrics_v1.keys()),\n",
    "#     INDEX_V1_NAME: list(metrics_v1.values()),\n",
    "#     INDEX_V3_NAME: list(metrics_v3.values())\n",
    "# })\n",
    "\n",
    "# # 성능 향상률 계산\n",
    "# comparison_df['Improvement (%)'] = ((comparison_df[INDEX_V3_NAME] - comparison_df[INDEX_V1_NAME]) \n",
    "#                                      / comparison_df[INDEX_V1_NAME] * 100)\n",
    "\n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(\"QnA 인덱스 성능 비교\")\n",
    "# print(\"=\"*80)\n",
    "# print(comparison_df.to_string(index=False))\n",
    "# print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8e6a10",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2323aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임베딩 모델 함수 구현 완료\n",
      "  - OpenAI text-embedding-3-small (dimension: 1536)\n",
      "  - jhgan/ko-sroberta-multitask (dimension: 768)\n"
     ]
    }
   ],
   "source": [
    "# # 임베딩 모델 구현\n",
    "\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# from typing import List\n",
    "# import torch\n",
    "\n",
    "# # 1) OpenAI 임베딩 함수 (기존 embeddings 객체 활용)\n",
    "# def embed_with_openai(texts: List[str], batch_size: int = 100) -> List[List[float]]:\n",
    "#     \"\"\"OpenAI text-embedding-3-small 모델로 임베딩 생성\"\"\"\n",
    "#     all_embeddings = []\n",
    "#     for i in range(0, len(texts), batch_size):\n",
    "#         batch = texts[i:i + batch_size]\n",
    "#         batch_embeddings = embeddings.embed_documents(batch)\n",
    "#         all_embeddings.extend(batch_embeddings)\n",
    "#     return all_embeddings\n",
    "\n",
    "# # 2) 한국어 임베딩 함수\n",
    "# def embed_with_korean_model(texts: List[str], batch_size: int = 32) -> List[List[float]]:\n",
    "#     \"\"\"jhgan/ko-sroberta-multitask 모델로 임베딩 생성\"\"\"\n",
    "#     if not hasattr(embed_with_korean_model, 'model'):\n",
    "#         print(\"한국어 임베딩 모델 로딩 중... (jhgan/ko-sroberta-multitask)\")\n",
    "#         embed_with_korean_model.model = SentenceTransformer('jhgan/ko-sroberta-multitask')\n",
    "#         if torch.cuda.is_available():\n",
    "#             embed_with_korean_model.model = embed_with_korean_model.model.to('cuda')\n",
    "#             print(\"  GPU로 실행\")\n",
    "#         else:\n",
    "#             print(\"  CPU로 실행\")\n",
    "    \n",
    "#     model = embed_with_korean_model.model\n",
    "#     all_embeddings = []\n",
    "#     for i in range(0, len(texts), batch_size):\n",
    "#         batch = texts[i:i + batch_size]\n",
    "#         batch_embeddings = model.encode(batch, convert_to_numpy=True, show_progress_bar=False)\n",
    "#         all_embeddings.extend(batch_embeddings.tolist())\n",
    "#     return all_embeddings\n",
    "\n",
    "# # 3) 임베딩 모델 래퍼 클래스\n",
    "# class EmbeddingModel:\n",
    "#     \"\"\"임베딩 모델 래퍼 클래스\"\"\"\n",
    "#     def __init__(self, model_name: str):\n",
    "#         self.model_name = model_name\n",
    "#         self.dimension = 1536 if model_name == 'openai' else 768\n",
    "        \n",
    "#     def embed(self, texts: List[str]) -> List[List[float]]:\n",
    "#         if self.model_name == 'openai':\n",
    "#             return embed_with_openai(texts)\n",
    "#         elif self.model_name == 'sroberta':\n",
    "#             return embed_with_korean_model(texts)\n",
    "#         else:\n",
    "#             raise ValueError(f\"Unknown model: {self.model_name}\")\n",
    "    \n",
    "#     def embed_query(self, query: str) -> List[float]:\n",
    "#         if self.model_name == 'openai':\n",
    "#             return embeddings.embed_query(query)\n",
    "#         elif self.model_name == 'sroberta':\n",
    "#             if not hasattr(embed_with_korean_model, 'model'):\n",
    "#                 embed_with_korean_model([query])\n",
    "#             return embed_with_korean_model.model.encode([query], convert_to_numpy=True)[0].tolist()\n",
    "#         else:\n",
    "#             raise ValueError(f\"Unknown model: {self.model_name}\")\n",
    "\n",
    "# print(\"임베딩 모델 함수 구현 완료\")\n",
    "# print(\"  - OpenAI text-embedding-3-small (dimension: 1536)\")\n",
    "# print(\"  - jhgan/ko-sroberta-multitask (dimension: 768)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46725ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 한국어 모델 인덱스 생성 및 데이터 적재\n",
    "\n",
    "# from pinecone import ServerlessSpec\n",
    "\n",
    "# print(\"=\" * 80)\n",
    "# print(\"한국어 모델 인덱스 생성 및 데이터 적재\")\n",
    "# print(\"=\" * 80)\n",
    "\n",
    "# # 한국어 모델용 새 인덱스 (dimension이 768로 다르므로 별도 인덱스 필요)\n",
    "# KOREAN_INDEX_NAME = \"plant-qna-v2\"\n",
    "# KOREAN_DIMENSION = 768\n",
    "# KOREAN_NAMESPACE = f\"{KOREAN_INDEX_NAME}-sroberta\"\n",
    "\n",
    "# print(f\"\\n적재 대상 문서 수: {len(docs_to_upsert)}개\")\n",
    "\n",
    "# # 한국어 모델용 인덱스 생성/로드\n",
    "# if KOREAN_INDEX_NAME not in [idx[\"name\"] for idx in pc.list_indexes()]:\n",
    "#     print(f\"\\n'{KOREAN_INDEX_NAME}' 인덱스 생성 중...\")\n",
    "#     pc.create_index(\n",
    "#         name=KOREAN_INDEX_NAME,\n",
    "#         dimension=KOREAN_DIMENSION,\n",
    "#         metric='cosine',\n",
    "#         spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "#     )\n",
    "#     print(f\"'{KOREAN_INDEX_NAME}' 인덱스 생성 완료\")\n",
    "# else:\n",
    "#     print(f\"\\n'{KOREAN_INDEX_NAME}' 인덱스 이미 존재\")\n",
    "\n",
    "# korean_index = pc.Index(KOREAN_INDEX_NAME)\n",
    "# print(f\"인덱스 연결 완료\")\n",
    "\n",
    "# # 기존 데이터 확인\n",
    "# existing_stats = korean_index.describe_index_stats()\n",
    "# print(f\"\\n현재 인덱스 상태:\")\n",
    "# print(f\"  - 총 벡터 수: {existing_stats.get('total_vector_count', 0)}\")\n",
    "# print(f\"  - 네임스페이스: {list(existing_stats.get('namespaces', {}).keys())}\")\n",
    "\n",
    "# # 사용자 확인\n",
    "# print(f\"\\n적재 설정:\")\n",
    "# print(f\"  - 인덱스: {KOREAN_INDEX_NAME}\")\n",
    "# print(f\"  - 네임스페이스: {KOREAN_NAMESPACE}\")\n",
    "# print(f\"  - Dimension: {KOREAN_DIMENSION}\")\n",
    "# print(f\"  - 문서 수: {len(docs_to_upsert)}개\")\n",
    "\n",
    "# response = input(\"\\n한국어 모델 임베딩을 생성하고 적재하시겠습니까? (y/n): \")\n",
    "\n",
    "# if response.lower() == 'y':\n",
    "#     print(\"\\n한국어 임베딩 생성 시작...\")\n",
    "    \n",
    "#     texts_to_embed = []\n",
    "#     for doc in docs_to_upsert:\n",
    "#         text = f\"Question: {doc['question']}\\nAnswer: {doc['answer']}\"\n",
    "#         texts_to_embed.append(text)\n",
    "    \n",
    "#     korean_model = EmbeddingModel('sroberta')\n",
    "#     vectors = korean_model.embed(texts_to_embed)\n",
    "#     print(f\"\\n임베딩 완료: {len(vectors)}개\")\n",
    "    \n",
    "#     insert_data = []\n",
    "#     for doc, vector in zip(docs_to_upsert, vectors):\n",
    "#         metadata = {\n",
    "#             'ids': doc['ids'],\n",
    "#             'text': f\"Question: {doc['question']}\\nAnswer: {doc['answer']}\",\n",
    "#             'question': doc['question'],\n",
    "#             'answer': doc['answer'][:500],\n",
    "#             'category_main': doc['category_main'],\n",
    "#             'category_sub': doc.get('category_sub', []),\n",
    "#             'post_id': doc['base_metadata']['post_id'],\n",
    "#             'channel': doc['base_metadata']['channel'],\n",
    "#             'date': doc['base_metadata']['date'],\n",
    "#             'source_platform': doc['base_metadata']['source_platform']\n",
    "#         }\n",
    "#         insert_data.append({'id': doc['ids'], 'values': vector, 'metadata': metadata})\n",
    "    \n",
    "#     def batch(iterable, n=100):\n",
    "#         for idx in range(0, len(iterable), n):\n",
    "#             yield iterable[idx: idx + n]\n",
    "    \n",
    "#     print(f\"\\nPinecone에 배치 업로드 중... (namespace: {KOREAN_NAMESPACE})\")\n",
    "#     for idx, batch_data in enumerate(batch(insert_data, 100)):\n",
    "#         korean_index.upsert(vectors=batch_data, namespace=KOREAN_NAMESPACE)\n",
    "#         print(f\"  Batch {idx+1}/{(len(insert_data)-1)//100 + 1} 완료 ({len(batch_data)}개)\")\n",
    "    \n",
    "#     print(f\"\\n총 {len(insert_data)}개 문서 적재 완료!\")\n",
    "#     print(f\"   네임스페이스: {KOREAN_NAMESPACE}\")\n",
    "    \n",
    "#     updated_stats = korean_index.describe_index_stats()\n",
    "#     print(f\"\\n업데이트된 인덱스 상태:\")\n",
    "#     print(f\"  - 총 벡터 수: {updated_stats.get('total_vector_count', 0)}\")\n",
    "#     for ns, info in updated_stats.get('namespaces', {}).items():\n",
    "#         print(f\"  - {ns}: {info.get('vector_count', 0)}개\")\n",
    "# else:\n",
    "#     print(\"\\n 적재를 건너뜁니다.\")\n",
    "\n",
    "# print(\"\\n\" + \"=\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
